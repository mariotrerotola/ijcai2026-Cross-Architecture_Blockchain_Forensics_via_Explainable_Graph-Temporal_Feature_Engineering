{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scam Token Detection - 3 Datasets x 3 Models (Balanced RF / MLP / SVM)\n",
    "\n",
    "This notebook evaluates **3 models** across **3 datasets**:\n",
    "\n",
    "- **Datasets**\n",
    "  - **ChainAbuse (scam) + CoinMarketCap (licit)**\n",
    "  - **TokenScout (scam) + TokenScout (licit)** *(heavily imbalanced, handled via SMOTE)*\n",
    "  - **Combined (ChainAbuse + TokenScout + CMC)** *(balanced via downsampling)*\n",
    "\n",
    "- **Models**\n",
    "  - **Balanced Random Forest** (`imblearn.ensemble.BalancedRandomForestClassifier`)\n",
    "  - **MLP** (`sklearn.neural_network.MLPClassifier`)\n",
    "  - **SVM** (`sklearn.svm.SVC`)\n",
    "\n",
    "## Efficiency Notes\n",
    "- TokenScout scam dataset is large: we use **sampling + SMOTE** to keep training time reasonable.\n",
    "- Preprocessing and feature selection are computed **once per dataset** and reused across all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scam Token Detection - SVM and MLP on ChainAbuse + CoinMarketCap\n",
    "\n",
    "This notebook runs SVM and MLP classifiers with:\n",
    "- Algorithms: SVM (RBF kernel) and Multi-Layer Perceptron\n",
    "- Data sources: ChainAbuse (scam) + CoinMarketCap (licit)\n",
    "\n",
    "Features:\n",
    "- Focused dataset (ChainAbuse + CoinMarketCap only)\n",
    "- SVM with RBF kernel\n",
    "- MLP with adaptive architecture\n",
    "- SMOTE for class balancing\n",
    "- Evaluation metrics and model comparison\n",
    "- SHAP interpretability (for MLP)\n",
    "\n",
    "Training time (approx):\n",
    "- SVM: ~10-20 minutes\n",
    "- MLP: ~5-10 minutes\n",
    "- Total: ~15-30 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Plotting settings\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR_CANDIDATES = [\n",
    "    '../data/dataset_with_features',  # if running from models/\n",
    "    './data/dataset_with_features',   # if running from project root\n",
    "    'data/dataset_with_features',\n",
    "]\n",
    "DATA_DIR = next((p for p in DATA_DIR_CANDIDATES if os.path.exists(p)), DATA_DIR_CANDIDATES[0])\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"Could not find dataset_with_features. Tried: {DATA_DIR_CANDIDATES}\")\n",
    "\n",
    "OUTPUT_DIR = './results_eval_3datasets_brf_mlp_svm'\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1  # From train+val after test split\n",
    "\n",
    "# Feature selection\n",
    "VARIANCE_THRESHOLD = 0.001\n",
    "CORRELATION_THRESHOLD = 0.95\n",
    "\n",
    "# Dataset balancing (for efficiency + fairness)\n",
    "BALANCE_TOKENSCOUT_DATASET = True\n",
    "BALANCE_COMBINED_DATASET = True\n",
    "\n",
    "# For combined-balanced: how to sample scams from the two scam sources\n",
    "COMBINED_SCAM_SOURCE_WEIGHTS = {\n",
    "    'chainabuse_scam_tokens': 0.5,\n",
    "    'tokenscout_scam_tokens': 0.5,\n",
    "}\n",
    "\n",
    "# Balanced Random Forest configuration\n",
    "BRF_PARAMS = {\n",
    "    'n_estimators': 400,\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "}\n",
    "\n",
    "# SVM Configuration\n",
    "SVM_PARAMS = {\n",
    "    'C': 10.0,\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 'scale',\n",
    "    'class_weight': 'balanced',\n",
    "    'probability': True,\n",
    "    'random_state': SEED,\n",
    "    'verbose': False,\n",
    "    'max_iter': 5000,\n",
    "    'cache_size': 2000,\n",
    "}\n",
    "\n",
    "# MLP Configuration\n",
    "MLP_PARAMS = {\n",
    "    'hidden_layer_sizes': (128, 64, 32),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 'adaptive',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'max_iter': 500,\n",
    "    'early_stopping': True,\n",
    "    'validation_fraction': 0.1,\n",
    "    'n_iter_no_change': 20,\n",
    "    'random_state': SEED,\n",
    "    'verbose': False,\n",
    "}\n",
    "\n",
    "# SMOTE configuration (only used for unbalanced datasets like ChainAbuse+CMC)\n",
    "SMOTE_PARAMS = {\n",
    "    'sampling_strategy': 0.8,\n",
    "    'random_state': SEED,\n",
    "    'k_neighbors': 5,\n",
    "}\n",
    "\n",
    "# Source paths\n",
    "CHAINABUSE_PATH = f'{DATA_DIR}/chainabuse_scam_tokens/features.csv'\n",
    "CMC_PATH = f'{DATA_DIR}/cmc_licit_tokens/features.csv'\n",
    "TOKENSCOUT_SCAM_PATH = f'{DATA_DIR}/tokenscout_scam_tokens/features.csv'\n",
    "TOKENSCOUT_LICIT_PATH = f'{DATA_DIR}/tokenscout_licit_tokens/features.csv'\n",
    "\n",
    "np.random.seed(SEED)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURATION - 3 DATASETS x 3 MODELS (BRF / MLP / SVM)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Random Seed: {SEED}\")\n",
    "print(f\"Test Size: {TEST_SIZE * 100:.0f}%\")\n",
    "print(f\"Validation Size: {VAL_SIZE * 100:.0f}% (from train+val after test split)\")\n",
    "print(\"\\nDatasets:\")\n",
    "print(\"  1) ChainAbuse (scam) + CMC (licit)          [SMOTE for MLP/SVM]\")\n",
    "print(\"  2) TokenScout (scam/licit)                 [balanced via downsampling]\")\n",
    "print(\"  3) Combined (ChainAbuse + TokenScout + CMC) [balanced via downsampling]\")\n",
    "print(\"\\nModels:\")\n",
    "print(f\"  - BalancedRF: n_estimators={BRF_PARAMS['n_estimators']}\")\n",
    "print(f\"  - SVM: kernel={SVM_PARAMS['kernel']}, C={SVM_PARAMS['C']}\")\n",
    "print(f\"  - MLP: layers={MLP_PARAMS['hidden_layer_sizes']}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading source feature files...\\n\")\n",
    "\n",
    "def read_features_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a features.csv file (try faster engine when available).\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, engine='pyarrow')\n",
    "    except Exception:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "SOURCE_SPECS = {\n",
    "    'chainabuse_scam_tokens': {'path': CHAINABUSE_PATH, 'target': 1},\n",
    "    'cmc_licit_tokens': {'path': CMC_PATH, 'target': 0},\n",
    "    'tokenscout_scam_tokens': {'path': TOKENSCOUT_SCAM_PATH, 'target': 1},\n",
    "    'tokenscout_licit_tokens': {'path': TOKENSCOUT_LICIT_PATH, 'target': 0},\n",
    "}\n",
    "\n",
    "SOURCE_DFS = {}\n",
    "for source_name, spec in SOURCE_SPECS.items():\n",
    "    path = spec['path']\n",
    "    print(f\"- Loading {source_name}: {path}\")\n",
    "    df_src = read_features_csv(path)\n",
    "    df_src['source_directory'] = source_name\n",
    "    df_src['target'] = spec['target']\n",
    "    SOURCE_DFS[source_name] = df_src\n",
    "    print(f\"  rows: {len(df_src):,} | columns: {df_src.shape[1]:,} | target={spec['target']}\")\n",
    "\n",
    "print(\"\\nAll sources loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBuilding evaluation datasets...\\n\")\n",
    "\n",
    "def downsample(df_in: pd.DataFrame, n: int, seed: int = SEED) -> pd.DataFrame:\n",
    "    if n is None or n >= len(df_in):\n",
    "        return df_in.copy()\n",
    "    return df_in.sample(n=n, random_state=seed)\n",
    "\n",
    "# 1) ChainAbuse (scam) + CMC (licit) - original distribution\n",
    "df_chainabuse_cmc = pd.concat(\n",
    "    [SOURCE_DFS['chainabuse_scam_tokens'], SOURCE_DFS['cmc_licit_tokens']],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# 2) TokenScout fraud/licit - use all data (no downsampling); SMOTE will balance during training\n",
    "_df_ts_scam = SOURCE_DFS['tokenscout_scam_tokens']\n",
    "_df_ts_licit = SOURCE_DFS['tokenscout_licit_tokens']\n",
    "\n",
    "# Concatenate all TokenScout data without balancing (heavily imbalanced: ~142k scam vs ~1.8k licit)\n",
    "df_tokenscout = pd.concat([_df_ts_scam, _df_ts_licit], ignore_index=True)\n",
    "\n",
    "# 3) Combined (ChainAbuse + TokenScout + CMC)\n",
    "df_licit_combined = pd.concat(\n",
    "    [SOURCE_DFS['cmc_licit_tokens'], SOURCE_DFS['tokenscout_licit_tokens']],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "df_scam_chainabuse = SOURCE_DFS['chainabuse_scam_tokens']\n",
    "df_scam_tokenscout = SOURCE_DFS['tokenscout_scam_tokens']\n",
    "\n",
    "if BALANCE_COMBINED_DATASET:\n",
    "    n_target = len(df_licit_combined)\n",
    "\n",
    "    w_chainabuse = float(COMBINED_SCAM_SOURCE_WEIGHTS.get('chainabuse_scam_tokens', 0.5))\n",
    "    w_tokenscout = float(COMBINED_SCAM_SOURCE_WEIGHTS.get('tokenscout_scam_tokens', 0.5))\n",
    "    w_sum = (w_chainabuse + w_tokenscout) or 1.0\n",
    "    w_chainabuse /= w_sum\n",
    "    w_tokenscout /= w_sum\n",
    "\n",
    "    n_chainabuse = int(round(n_target * w_chainabuse))\n",
    "    n_tokenscout = n_target - n_chainabuse\n",
    "\n",
    "    df_scam_sample = pd.concat(\n",
    "        [downsample(df_scam_chainabuse, n_chainabuse), downsample(df_scam_tokenscout, n_tokenscout)],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    df_combined = pd.concat([df_licit_combined, df_scam_sample], ignore_index=True)\n",
    "else:\n",
    "    df_combined = pd.concat([df_licit_combined, df_scam_chainabuse, df_scam_tokenscout], ignore_index=True)\n",
    "\n",
    "# Shuffle (nicer splits)\n",
    "df_chainabuse_cmc = df_chainabuse_cmc.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "df_tokenscout = df_tokenscout.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "df_combined = df_combined.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "DATASETS = {\n",
    "    'chainabuse+cmc': {'df': df_chainabuse_cmc, 'use_smote': True},\n",
    "    'tokenscout_fraud_licit': {'df': df_tokenscout, 'use_smote': True},  # Now uses SMOTE (all 144k samples)\n",
    "    'combined_balanced': {'df': df_combined, 'use_smote': False},\n",
    "}\n",
    "\n",
    "def dataset_overview(name: str, df_in: pd.DataFrame) -> None:\n",
    "    counts = df_in['target'].value_counts().sort_index()\n",
    "    sources = df_in['source_directory'].value_counts()\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Samples: {len(df_in):,}\")\n",
    "    print(f\"Licit (0): {int(counts.get(0, 0)):,}\")\n",
    "    print(f\"Scam  (1): {int(counts.get(1, 0)):,}\")\n",
    "    if int(counts.get(0, 0)) > 0:\n",
    "        print(f\"Imbalance ratio (scam/lict): {int(counts.get(1, 0)) / int(counts.get(0, 0)):.2f}\")\n",
    "    print(\"Sources:\")\n",
    "    print(sources)\n",
    "\n",
    "for name, spec in DATASETS.items():\n",
    "    dataset_overview(name, spec['df'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Validation/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining + evaluating models on all datasets...\\n\")\n",
    "\n",
    "# Threshold grid for validation tuning\n",
    "THRESHOLDS = np.arange(0.1, 0.91, 0.01)\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = print\n",
    "\n",
    "\n",
    "def prepare_Xy(df_in: pd.DataFrame):\n",
    "    # Keep numeric features only\n",
    "    exclude_cols = {'target', 'label', 'source_directory', 'token_address', 'token_file'}\n",
    "    drop_cols = [c for c in df_in.columns if c in exclude_cols]\n",
    "    X_df = df_in.drop(columns=drop_cols, errors='ignore')\n",
    "    X_df = X_df.select_dtypes(include=[np.number]).copy()\n",
    "    y = df_in['target'].astype(int).values\n",
    "    return X_df, y\n",
    "\n",
    "\n",
    "def fit_transform_feature_selection(X_train_df: pd.DataFrame, X_val_df: pd.DataFrame, X_test_df: pd.DataFrame):\n",
    "    \"\"\"Impute + variance threshold + correlation filter (fit only on train).\"\"\"\n",
    "\n",
    "    # 1) Impute\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imp = imputer.fit_transform(X_train_df)\n",
    "    X_val_imp = imputer.transform(X_val_df)\n",
    "    X_test_imp = imputer.transform(X_test_df)\n",
    "\n",
    "    # 2) Replace inf/-inf with finite values + clip to float32-safe range\n",
    "    MAX_VAL = 1e10  # Safe for float32 (max ~3.4e38)\n",
    "    X_train_imp = np.nan_to_num(X_train_imp, nan=0.0, posinf=MAX_VAL, neginf=-MAX_VAL)\n",
    "    X_val_imp = np.nan_to_num(X_val_imp, nan=0.0, posinf=MAX_VAL, neginf=-MAX_VAL)\n",
    "    X_test_imp = np.nan_to_num(X_test_imp, nan=0.0, posinf=MAX_VAL, neginf=-MAX_VAL)\n",
    "    \n",
    "    # Extra safety: clip any remaining extreme values\n",
    "    X_train_imp = np.clip(X_train_imp, -MAX_VAL, MAX_VAL)\n",
    "    X_val_imp = np.clip(X_val_imp, -MAX_VAL, MAX_VAL)\n",
    "    X_test_imp = np.clip(X_test_imp, -MAX_VAL, MAX_VAL)\n",
    "\n",
    "    # 3) Variance threshold\n",
    "    var_sel = VarianceThreshold(threshold=VARIANCE_THRESHOLD)\n",
    "    X_train_var = var_sel.fit_transform(X_train_imp)\n",
    "    X_val_var = var_sel.transform(X_val_imp)\n",
    "    X_test_var = var_sel.transform(X_test_imp)\n",
    "\n",
    "    var_feature_names = list(X_train_df.columns[var_sel.get_support()])\n",
    "\n",
    "    # 4) Correlation filter\n",
    "    df_train_var = pd.DataFrame(X_train_var, columns=var_feature_names)\n",
    "    corr = df_train_var.corr().abs().fillna(0.0)\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [c for c in upper.columns if any(upper[c] > CORRELATION_THRESHOLD)]\n",
    "\n",
    "    keep_features = [c for c in var_feature_names if c not in to_drop]\n",
    "    keep_idx = [var_feature_names.index(c) for c in keep_features]\n",
    "\n",
    "    X_train_sel = X_train_var[:, keep_idx]\n",
    "    X_val_sel = X_val_var[:, keep_idx]\n",
    "    X_test_sel = X_test_var[:, keep_idx]\n",
    "\n",
    "    meta = {\n",
    "        'imputer': imputer,\n",
    "        'var_selector': var_sel,\n",
    "        'dropped_correlated': to_drop,\n",
    "        'feature_names': keep_features,\n",
    "    }\n",
    "    return X_train_sel, X_val_sel, X_test_sel, meta\n",
    "\n",
    "\n",
    "def optimize_threshold(y_true: np.ndarray, y_score: np.ndarray, thresholds=THRESHOLDS):\n",
    "    best_t = 0.5\n",
    "    best = -1.0\n",
    "    for t in thresholds:\n",
    "        pred = (y_score >= t).astype(int)\n",
    "        score = balanced_accuracy_score(y_true, pred)\n",
    "        if score > best:\n",
    "            best = float(score)\n",
    "            best_t = float(t)\n",
    "    return best_t, best\n",
    "\n",
    "\n",
    "def fit_eval_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    t0 = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    val_score = model.predict_proba(X_val)[:, 1]\n",
    "    best_t, best_val_balacc = optimize_threshold(y_val, val_score)\n",
    "\n",
    "    test_score = model.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_score >= best_t).astype(int)\n",
    "\n",
    "    return {\n",
    "        'train_time_s': float(train_time),\n",
    "        'threshold': float(best_t),\n",
    "        'val_bal_acc': float(best_val_balacc),\n",
    "        'test_bal_acc': float(balanced_accuracy_score(y_test, test_pred)),\n",
    "        'test_roc_auc': float(roc_auc_score(y_test, test_score)),\n",
    "        'test_pr_auc': float(average_precision_score(y_test, test_score)),\n",
    "        'test_mcc': float(matthews_corrcoef(y_test, test_pred)),\n",
    "        'test_f1': float(f1_score(y_test, test_pred)),\n",
    "        'cm': confusion_matrix(y_test, test_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "results = []\n",
    "confusion_matrices = {}  # (dataset, model) -> confusion matrix\n",
    "\n",
    "for ds_name, ds_spec in DATASETS.items():\n",
    "    df_ds = ds_spec['df']\n",
    "    use_smote = bool(ds_spec.get('use_smote', False))\n",
    "\n",
    "    X_df, y = prepare_Xy(df_ds)\n",
    "\n",
    "    # Train/val/test split (per-dataset)\n",
    "    X_temp, X_test_df, y_temp, y_test = train_test_split(\n",
    "        X_df, y, test_size=TEST_SIZE, random_state=SEED, stratify=y\n",
    "    )\n",
    "    X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # Feature selection (fit only on train)\n",
    "    X_train_sel, X_val_sel, X_test_sel, fs_meta = fit_transform_feature_selection(\n",
    "        X_train_df, X_val_df, X_test_df\n",
    "    )\n",
    "\n",
    "    # Scale for MLP/SVM\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sel)\n",
    "    X_val_scaled = scaler.transform(X_val_sel)\n",
    "    X_test_scaled = scaler.transform(X_test_sel)\n",
    "\n",
    "    # SMOTE only for MLP/SVM when requested\n",
    "    if use_smote:\n",
    "        smote = SMOTE(**SMOTE_PARAMS)\n",
    "        X_train_scaled_fit, y_train_fit = smote.fit_resample(X_train_scaled, y_train)\n",
    "    else:\n",
    "        X_train_scaled_fit, y_train_fit = X_train_scaled, y_train\n",
    "\n",
    "    models = {\n",
    "        'BalancedRF': BalancedRandomForestClassifier(**BRF_PARAMS),\n",
    "        'MLP': MLPClassifier(**MLP_PARAMS),\n",
    "        'SVM': SVC(**SVM_PARAMS),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATASET: {ds_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Samples: {len(df_ds):,} | raw numeric features: {X_df.shape[1]:,} | selected: {len(fs_meta['feature_names']):,}\")\n",
    "    print(f\"Train/Val/Test: {len(y_train):,}/{len(y_val):,}/{len(y_test):,}\")\n",
    "    if use_smote:\n",
    "        print(f\"SMOTE (train only, for MLP/SVM): {len(y_train):,} -> {len(y_train_fit):,}\")\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        if model_name == 'BalancedRF':\n",
    "            res = fit_eval_model(model, X_train_sel, y_train, X_val_sel, y_val, X_test_sel, y_test)\n",
    "            train_rows_fit = len(y_train)\n",
    "            smote_used = False\n",
    "        else:\n",
    "            res = fit_eval_model(model, X_train_scaled_fit, y_train_fit, X_val_scaled, y_val, X_test_scaled, y_test)\n",
    "            train_rows_fit = len(y_train_fit)\n",
    "            smote_used = use_smote\n",
    "\n",
    "        confusion_matrices[(ds_name, model_name)] = res['cm']\n",
    "\n",
    "        results.append({\n",
    "            'dataset': ds_name,\n",
    "            'model': model_name,\n",
    "            'n_samples': int(len(df_ds)),\n",
    "            'n_features_selected': int(len(fs_meta['feature_names'])),\n",
    "            'train_rows_fit': int(train_rows_fit),\n",
    "            'smote': bool(smote_used),\n",
    "            'threshold': res['threshold'],\n",
    "            'val_bal_acc': res['val_bal_acc'],\n",
    "            'test_bal_acc': res['test_bal_acc'],\n",
    "            'test_roc_auc': res['test_roc_auc'],\n",
    "            'test_pr_auc': res['test_pr_auc'],\n",
    "            'test_mcc': res['test_mcc'],\n",
    "            'test_f1': res['test_f1'],\n",
    "            'train_time_s': res['train_time_s'],\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"- {model_name:10s} | bal_acc={res['test_bal_acc']:.4f} | roc_auc={res['test_roc_auc']:.4f} | \"\n",
    "            f\"pr_auc={res['test_pr_auc']:.4f} | f1={res['test_f1']:.4f} | t={res['threshold']:.2f} | \"\n",
    "            f\"train={res['train_time_s']:.1f}s\"\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(['dataset', 'model']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY (test metrics)\")\n",
    "print(f\"{'='*80}\")\n",
    "display(results_df)\n",
    "\n",
    "balacc_pivot = results_df.pivot(index='dataset', columns='model', values='test_bal_acc')\n",
    "print(\"\\nBalanced Accuracy (test) - pivot:\")\n",
    "display(balacc_pivot)\n",
    "\n",
    "results_path = os.path.join(OUTPUT_DIR, 'results_3datasets_brf_mlp_svm.csv')\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nSaved results to: {results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train SVM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' not in globals():\n",
    "    raise RuntimeError(\"Run the previous cell first to generate results_df\")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=results_df, x='dataset', y='test_bal_acc', hue='model')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title('Test Balanced Accuracy - 3 datasets x 3 models')\n",
    "plt.ylabel('Balanced Accuracy (test)')\n",
    "plt.xlabel('Dataset')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(OUTPUT_DIR, 'balanced_accuracy_comparison.png')\n",
    "plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved plot to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train MLP Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' not in globals() or 'confusion_matrices' not in globals():\n",
    "    raise RuntimeError(\"Run the evaluation cell first to generate results_df + confusion_matrices\")\n",
    "\n",
    "datasets = list(results_df['dataset'].unique())\n",
    "models = list(results_df['model'].unique())\n",
    "\n",
    "fig, axes = plt.subplots(len(datasets), len(models), figsize=(4.2 * len(models), 3.6 * len(datasets)))\n",
    "\n",
    "# Make axes always 2D\n",
    "if len(datasets) == 1 and len(models) == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif len(datasets) == 1:\n",
    "    axes = np.array([axes])\n",
    "elif len(models) == 1:\n",
    "    axes = np.array([[ax] for ax in axes])\n",
    "\n",
    "for i, ds in enumerate(datasets):\n",
    "    for j, model in enumerate(models):\n",
    "        ax = axes[i, j]\n",
    "        cm = confusion_matrices.get((ds, model))\n",
    "        if cm is None:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            cbar=False,\n",
    "            ax=ax,\n",
    "            xticklabels=['Licit', 'Scam'],\n",
    "            yticklabels=['Licit', 'Scam'],\n",
    "        )\n",
    "        ax.set_title(f\"{ds}\\n{model}\")\n",
    "        ax.set_xlabel('Pred')\n",
    "        ax.set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(OUTPUT_DIR, 'confusion_matrices_grid.png')\n",
    "plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved confusion matrix grid to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' not in globals():\n",
    "    raise RuntimeError(\"Run the evaluation cell first to generate results_df\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = print\n",
    "\n",
    "cols = [\n",
    "    'dataset',\n",
    "    'model',\n",
    "    'threshold',\n",
    "    'val_bal_acc',\n",
    "    'test_bal_acc',\n",
    "    'test_roc_auc',\n",
    "    'test_pr_auc',\n",
    "    'test_f1',\n",
    "]\n",
    "\n",
    "display(results_df[cols].sort_values(['dataset', 'model']).reset_index(drop=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
