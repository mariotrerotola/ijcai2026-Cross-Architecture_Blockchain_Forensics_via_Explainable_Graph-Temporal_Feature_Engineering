{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scam Token Detection — Balanced Random Forest Hyperparameter Tuning\n",
    "\n",
    "This notebook implements a complete hyperparameter tuning pipeline for detecting scam tokens.\n",
    "\n",
    "**Key components:**\n",
    "- **Dataset**: ChainAbuse (scam) + CoinMarketCap (licit)\n",
    "- **Model**: Balanced Random Forest\n",
    "- **GridSearch**: Hyperparameter optimization via cross-validation\n",
    "- **Threshold Tuning**: Decision threshold optimization on validation set\n",
    "- **SMOTE**: Synthetic oversampling to handle class imbalance\n",
    "\n",
    "## Goal\n",
    "Find the optimal Balanced Random Forest configuration to maximize balanced accuracy on imbalanced scam detection data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Plotting settings\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR_CANDIDATES = [\n",
    "    '../data/dataset_with_features',\n",
    "    './data/dataset_with_features',\n",
    "    'data/dataset_with_features',\n",
    "]\n",
    "DATA_DIR = next((p for p in DATA_DIR_CANDIDATES if os.path.exists(p)), DATA_DIR_CANDIDATES[0])\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"Could not find dataset_with_features. Tried: {DATA_DIR_CANDIDATES}\")\n",
    "\n",
    "OUTPUT_DIR = './results_brf_hyperparameter_tuning_final'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Data paths\n",
    "CHAINABUSE_PATH = f'{DATA_DIR}/chainabuse_scam_tokens/features.csv'\n",
    "CMC_PATH = f'{DATA_DIR}/cmc_licit_tokens/features.csv'\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Train/Val/Test split\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "# Feature selection\n",
    "VARIANCE_THRESHOLD = 0.001\n",
    "CORRELATION_THRESHOLD = 0.95\n",
    "\n",
    "# SMOTE configuration\n",
    "SMOTE_PARAMS = {\n",
    "    'sampling_strategy': 0.8,\n",
    "    'random_state': SEED,\n",
    "    'k_neighbors': 5,\n",
    "}\n",
    "\n",
    "# GridSearch CV configuration\n",
    "CV_FOLDS = 5\n",
    "N_JOBS = -1  # Use all CPU cores\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION — BALANCED RANDOM FOREST HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Random Seed: {SEED}\")\n",
    "print(f\"Test Size: {TEST_SIZE*100:.0f}%\")\n",
    "print(f\"Validation Size: {VAL_SIZE*100:.0f}%\")\n",
    "print(f\"GridSearch CV Folds: {CV_FOLDS}\")\n",
    "print(f\"SMOTE strategy: {SMOTE_PARAMS['sampling_strategy']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature Engineering & Selection...\\n\")\n",
    "\n",
    "# Extract features and target\n",
    "exclude_cols = {'target', 'label', 'source', 'source_directory', 'token_address', 'token_file'}\n",
    "drop_cols = [c for c in df.columns if c in exclude_cols]\n",
    "X_df = df.drop(columns=drop_cols, errors='ignore')\n",
    "X_df = X_df.select_dtypes(include=[np.number]).copy()\n",
    "y = df['target'].astype(int).values\n",
    "\n",
    "print(f\"Initial numeric features: {X_df.shape[1]}\")\n",
    "\n",
    "# Train/Val/Test split (stratified)\n",
    "X_temp, X_test_df, y_temp, y_test = train_test_split(\n",
    "    X_df, y, test_size=TEST_SIZE, random_state=SEED, stratify=y\n",
    ")\n",
    "X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Val/Test split:\")\n",
    "print(f\"  Train: {len(y_train):,} samples (scam={y_train.sum():,}, licit={len(y_train)-y_train.sum():,})\")\n",
    "print(f\"  Val:   {len(y_val):,} samples (scam={y_val.sum():,}, licit={len(y_val)-y_val.sum():,})\")\n",
    "print(f\"  Test:  {len(y_test):,} samples (scam={y_test.sum():,}, licit={len(y_test)-y_test.sum():,})\")\n",
    "\n",
    "# 1) Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imp = imputer.fit_transform(X_train_df)\n",
    "X_val_imp = imputer.transform(X_val_df)\n",
    "X_test_imp = imputer.transform(X_test_df)\n",
    "\n",
    "# 2) Replace inf/-inf with finite values\n",
    "MAX_VAL = 1e10\n",
    "X_train_imp = np.nan_to_num(X_train_imp, nan=0.0, posinf=MAX_VAL, neginf=-MAX_VAL)\n",
    "X_val_imp = np.nan_to_num(X_val_imp, nan=0.0, posinf=MAX_VAL, neginf=-MAX_VAL)\n",
    "X_test_imp = np.nan_to_num(X_test_imp, nan=0.0, posinf=MAX_VAL, neginf=-MAX_VAL)\n",
    "\n",
    "X_train_imp = np.clip(X_train_imp, -MAX_VAL, MAX_VAL)\n",
    "X_val_imp = np.clip(X_val_imp, -MAX_VAL, MAX_VAL)\n",
    "X_test_imp = np.clip(X_test_imp, -MAX_VAL, MAX_VAL)\n",
    "\n",
    "# 3) Variance threshold\n",
    "var_sel = VarianceThreshold(threshold=VARIANCE_THRESHOLD)\n",
    "X_train_var = var_sel.fit_transform(X_train_imp)\n",
    "X_val_var = var_sel.transform(X_val_imp)\n",
    "X_test_var = var_sel.transform(X_test_imp)\n",
    "\n",
    "var_feature_names = list(X_train_df.columns[var_sel.get_support()])\n",
    "print(f\"\\nAfter variance threshold ({VARIANCE_THRESHOLD}): {len(var_feature_names)} features\")\n",
    "\n",
    "# 4) Correlation filter\n",
    "df_train_var = pd.DataFrame(X_train_var, columns=var_feature_names)\n",
    "corr = df_train_var.corr().abs().fillna(0.0)\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > CORRELATION_THRESHOLD)]\n",
    "\n",
    "keep_features = [c for c in var_feature_names if c not in to_drop]\n",
    "keep_idx = [var_feature_names.index(c) for c in keep_features]\n",
    "\n",
    "X_train = X_train_var[:, keep_idx]\n",
    "X_val = X_val_var[:, keep_idx]\n",
    "X_test = X_test_var[:, keep_idx]\n",
    "\n",
    "print(f\"After correlation filter (>{CORRELATION_THRESHOLD}): {len(keep_features)} features\")\n",
    "print(f\"Total removed: {X_df.shape[1] - len(keep_features)} features\")\n",
    "print(\"\\n✓ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GridSearch Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"GRIDSEARCH HYPERPARAMETER TUNING\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Create pipeline with SMOTE + BalancedRandomForest\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(**SMOTE_PARAMS)),\n",
    "    ('classifier', BalancedRandomForestClassifier(random_state=SEED, n_jobs=N_JOBS))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [200, 400, 600],\n",
    "    'classifier__max_depth': [None, 20, 30, 50],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None],\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")\n",
    "print(f\"Total fits: {total_combinations * CV_FOLDS}\")\n",
    "print(f\"\\nUsing {CV_FOLDS}-fold cross-validation with balanced_accuracy scoring...\\n\")\n",
    "\n",
    "# GridSearch with balanced_accuracy scoring\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=2,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearch (this may take 30-60+ minutes)...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GridSearch complete in {elapsed/60:.1f} minutes\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV balanced accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_path = os.path.join(OUTPUT_DIR, 'gridsearch_results.csv')\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\n✓ GridSearch results saved to: {results_path}\")\n",
    "\n",
    "# Save best model\n",
    "best_model_path = os.path.join(OUTPUT_DIR, 'best_brf_model.pkl')\n",
    "joblib.dump(grid_search.best_estimator_, best_model_path)\n",
    "print(f\"✓ Best model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Threshold Tuning on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"THRESHOLD TUNING ON VALIDATION SET\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Get validation predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = np.arange(0.1, 0.91, 0.01)\n",
    "val_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_val_proba >= threshold).astype(int)\n",
    "    score = balanced_accuracy_score(y_val, y_pred)\n",
    "    val_scores.append(score)\n",
    "\n",
    "best_idx = np.argmax(val_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_val_score = val_scores[best_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.4f}\")\n",
    "print(f\"Validation balanced accuracy: {best_val_score:.4f}\")\n",
    "\n",
    "# Plot threshold vs balanced accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, val_scores, linewidth=2, color='steelblue')\n",
    "plt.axvline(best_threshold, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Optimal={best_threshold:.3f}')\n",
    "plt.xlabel('Threshold', fontsize=12)\n",
    "plt.ylabel('Balanced Accuracy', fontsize=12)\n",
    "plt.title('Threshold Tuning on Validation Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "threshold_plot_path = os.path.join(OUTPUT_DIR, 'threshold_tuning.png')\n",
    "plt.savefig(threshold_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Threshold tuning plot saved to: {threshold_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Set Evaluation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Get test predictions with optimal threshold\n",
    "y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_pr_auc = average_precision_score(y_test, y_test_proba)\n",
    "test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f\"  Balanced Accuracy: {test_bal_acc:.4f}\")\n",
    "print(f\"  ROC-AUC:          {test_roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:           {test_pr_auc:.4f}\")\n",
    "print(f\"  MCC:              {test_mcc:.4f}\")\n",
    "print(f\"  F1 Score:         {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Licit', 'Scam']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"  TN={cm[0,0]:,}, FP={cm[0,1]:,}\")\n",
    "print(f\"  FN={cm[1,0]:,}, TP={cm[1,1]:,}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'best_threshold': float(best_threshold),\n",
    "    'val_balanced_accuracy': float(best_val_score),\n",
    "    'test_balanced_accuracy': float(test_bal_acc),\n",
    "    'test_roc_auc': float(test_roc_auc),\n",
    "    'test_pr_auc': float(test_pr_auc),\n",
    "    'test_mcc': float(test_mcc),\n",
    "    'test_f1': float(test_f1),\n",
    "    'test_tn': int(cm[0,0]),\n",
    "    'test_fp': int(cm[0,1]),\n",
    "    'test_fn': int(cm[1,0]),\n",
    "    'test_tp': int(cm[1,1]),\n",
    "}\n",
    "\n",
    "metrics_path = os.path.join(OUTPUT_DIR, 'final_metrics.json')\n",
    "import json\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Final metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Plot confusion matrix + ROC + PR curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax1,\n",
    "            xticklabels=['Licit', 'Scam'], yticklabels=['Licit', 'Scam'], annot_kws={'fontsize': 14})\n",
    "ax1.set_title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# ROC Curve\n",
    "ax2 = axes[1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "ax2.plot(fpr, tpr, linewidth=2.5, color='steelblue', label=f'ROC (AUC={test_roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.3)\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax2.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11, loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax3 = axes[2]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "ax3.plot(recall, precision, linewidth=2.5, color='coral', label=f'PR (AUC={test_pr_auc:.3f})')\n",
    "ax3.set_xlabel('Recall', fontsize=12)\n",
    "ax3.set_ylabel('Precision', fontsize=12)\n",
    "ax3.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax3.legend(fontsize=11, loc='upper right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "viz_path = os.path.join(OUTPUT_DIR, 'evaluation_visualizations.png')\n",
    "plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualizations saved to: {viz_path}\")\n",
    "\n",
    "# FINAL SUMMARY\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Dataset: ChainAbuse (scam) + CoinMarketCap (licit)\")\n",
    "print(f\"Total samples: {len(df):,} | Features (after selection): {len(keep_features)}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nOptimal Decision Threshold: {best_threshold:.4f}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Validation Balanced Accuracy: {best_val_score:.4f}\")\n",
    "print(f\"  Test Balanced Accuracy:       {test_bal_acc:.4f}\")\n",
    "print(f\"  Test ROC-AUC:                 {test_roc_auc:.4f}\")\n",
    "print(f\"  Test PR-AUC:                  {test_pr_auc:.4f}\")\n",
    "print(f\"  Test F1 Score:                {test_f1:.4f}\")\n",
    "print(f\"  Test MCC:                     {test_mcc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scam and legitimate token datasets\n",
    "print(\"\\nLoading ChainAbuse + CoinMarketCap datasets...\\n\")\n",
    "\n",
    "def read_features_csv(path: str):\n",
    "    \"\"\"Read CSV with fast pyarrow engine if available, fallback to pandas.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, engine='pyarrow')\n",
    "    except Exception:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "# Load scam tokens from ChainAbuse\n",
    "print(f\"Loading ChainAbuse (scam): {CHAINABUSE_PATH}\")\n",
    "df_chainabuse = read_features_csv(CHAINABUSE_PATH)\n",
    "df_chainabuse['source'] = 'chainabuse_scam'\n",
    "df_chainabuse['target'] = 1\n",
    "print(f\"  Loaded: {len(df_chainabuse):,} rows\")\n",
    "\n",
    "# Load legitimate tokens from CoinMarketCap\n",
    "print(f\"Loading CoinMarketCap (licit): {CMC_PATH}\")\n",
    "df_cmc = read_features_csv(CMC_PATH)\n",
    "df_cmc['source'] = 'cmc_licit'\n",
    "df_cmc['target'] = 0\n",
    "print(f\"  Loaded: {len(df_cmc):,} rows\")\n",
    "\n",
    "# Combine and shuffle\n",
    "df = pd.concat([df_chainabuse, df_cmc], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Print dataset summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['target'].value_counts().sort_index())\n",
    "print(f\"\\nImbalance ratio (scam/licit): {df['target'].sum() / (len(df) - df['target'].sum()):.2f}\")\n",
    "print(f\"\\nSource distribution:\")\n",
    "print(df['source'].value_counts())\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
