{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b153ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Synthetic Data Augmentation, Student NN & Rule Extraction\n",
    "========================================================\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. **Teacher** Random Forest on the original train split.\n",
    "2. **SMOTE** synthetic generation up to `TARGET_TOTAL` samples.\n",
    "3. **Self‑label** synthetic data with the teacher and keep only high-confidence\n",
    "   points (≥ `CONF_THRESH`).\n",
    "4. Merge original + synthetic data → train **MLP student**.\n",
    "5. Evaluate the student on the untouched test split.\n",
    "6. **DEXiRE** rule extraction from the trained student for symbolic\n",
    "   interpretability, with a prettified mapping of feature indices to names.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c600adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Parameters & reproducibility\n",
    "# ----------------------------------------------------------------------\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras import callbacks, layers, models, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from dexire.dexire import DEXiRE  # pip install dexire\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "SEED           = 42\n",
    "CONF_THRESH    = 0.90       # teacher confidence threshold\n",
    "TEST_SIZE      = 0.20\n",
    "TARGET_TOTAL   = 10_000_000  # reduce to 2M for demo; adjust as needed\n",
    "DATA_PATH      = Path(os.getenv(\n",
    "    \"DATASET_PATH\",\n",
    "    \"../../data/processed_data/case_3.csv\",\n",
    "))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c179e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 1⃣  Load dataset & split\n",
    "# ----------------------------------------------------------------------\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found → {DATA_PATH.resolve()}\")\n",
    "\n",
    "print(\"[INFO] Loading dataset …\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df.drop(columns=[\"class\"]).values\n",
    "feature_names = df.drop(columns=[\"class\"]).columns  # keep for DEXiRE\n",
    "\n",
    "y = df[\"class\"].values\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=TEST_SIZE, stratify=y_enc, random_state=SEED\n",
    ")\n",
    "print(f\"[INFO] Original train distribution: {np.bincount(y_train)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ec266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 2⃣  Train teacher Random Forest\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"[INFO] Training RandomForest teacher …\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"[INFO] Teacher trained.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 3⃣  SMOTE synthetic generation & filtering\n",
    "# ----------------------------------------------------------------------\n",
    "classes           = np.unique(y_train)\n",
    "per_class_target  = TARGET_TOTAL // len(classes)\n",
    "smote_strategy    = {cls: per_class_target for cls in classes}\n",
    "print(f\"[INFO] SMOTE target per class: {smote_strategy}\")\n",
    "\n",
    "smote  = SMOTE(sampling_strategy=smote_strategy, random_state=SEED)\n",
    "print(\"[INFO] Generating synthetic samples …\")\n",
    "X_syn_full, _ = smote.fit_resample(X_train, y_train)\n",
    "synthetic_count = X_syn_full.shape[0] - X_train.shape[0]\n",
    "X_syn = X_syn_full[-synthetic_count:]\n",
    "\n",
    "print(\"[INFO] Teacher self‑labelling & confidence filter …\")\n",
    "proba_syn = rf.predict_proba(X_syn)\n",
    "conf_max  = proba_syn.max(axis=1)\n",
    "y_syn_lbl = proba_syn.argmax(axis=1)\n",
    "\n",
    "mask_conf   = conf_max >= CONF_THRESH\n",
    "X_syn_filt  = X_syn[mask_conf]\n",
    "y_syn_filt  = y_syn_lbl[mask_conf]\n",
    "print(f\"[INFO] High‑confidence synthetic kept: {len(y_syn_filt)} / {synthetic_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 4⃣  Merge & shuffle\n",
    "# ----------------------------------------------------------------------\n",
    "X_comb = np.vstack([X_train, X_syn_filt])\n",
    "y_comb = np.concatenate([y_train, y_syn_filt])\n",
    "perm    = np.random.permutation(len(y_comb))\n",
    "X_comb, y_comb = X_comb[perm], y_comb[perm]\n",
    "print(f\"[INFO] Final train size: {X_comb.shape}, distribution: {np.bincount(y_comb)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c39669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 5⃣  Scale & one‑hot labels\n",
    "# ----------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_comb_s = scaler.fit_transform(X_comb)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "y_comb_o = to_categorical(y_comb)\n",
    "y_test_o = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 6⃣  Build + train student MLP\n",
    "# ----------------------------------------------------------------------\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Assumes X_train, y_train, X_val, y_val, X_test, y_test are already prepared\n",
    "# and class_weight is defined if you want to balance classes\n",
    "\n",
    "# 1. Costruzione del modello\n",
    "\n",
    "\n",
    "\n",
    "def build_student(input_dim: int, output_dim: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        # Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        # Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        # Dropout(0.3),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "print(\"[INFO] Training student MLP …\")\n",
    "student = build_student(X_comb_s.shape[1], y_comb_o.shape[1])\n",
    "cb_es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "cb_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
    "student.fit(\n",
    "    X_comb_s,\n",
    "    y_comb_o,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=1024,\n",
    "    callbacks=[cb_es, cb_lr],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 7⃣  Evaluation\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"[INFO] Evaluating student …\")\n",
    "loss, acc = student.evaluate(X_test_s, y_test_o, verbose=0)\n",
    "print(f\"[RESULT] TEST – Loss: {loss:.4f} | Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "y_pred = np.argmax(student.predict(X_test_s, verbose=0), axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29127651",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = student.predict(X_comb_s[:1])  # oppure: student(X_comb_s[:1], training=False)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 8⃣  Symbolic Rule Extraction with DEXiRE\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"[INFO] Extracting rules with DEXiRE … (this may take a while)\")\n",
    "\n",
    "# Build DataFrame of scaled features for DEXiRE\n",
    "features_df = pd.DataFrame(X_comb_s, columns=feature_names)\n",
    "\n",
    "dexire = DEXiRE(model=student, class_names=le.classes_.tolist())\n",
    "rules_raw = dexire.extract_rules(features_df, y_comb)\n",
    "\n",
    "# Save raw rules\n",
    "rules_path = Path(\"rules_student.txt\")\n",
    "rules_path.write_text(str(rules_raw), encoding=\"utf-8\")\n",
    "print(f\"[INFO] Raw rules saved → {rules_path}\")\n",
    "\n",
    "# Prettify placeholders (X_i) with real feature names\n",
    "idx2name = {i: name for i, name in enumerate(feature_names)}\n",
    "\n",
    "def repl(match):\n",
    "    return idx2name.get(int(match.group(1)), match.group(0))\n",
    "\n",
    "rules_pretty = re.sub(r\"X_(\\d+)\", repl, str(rules_raw))\n",
    "pretty_path = Path(\"rules_student_pretty.txt\")\n",
    "pretty_path.write_text(rules_pretty, encoding=\"utf-8\")\n",
    "print(f\"[INFO] Pretty rules saved → {pretty_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dexire-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
