{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bitcoin Fraud Detection using Multi-Layer Perceptron (MLP)\n",
    "\n",
    "This notebook trains a neural network classifier for wallet fraud detection.\n",
    "We perform a simple hyperparameter search over network architectures and\n",
    "evaluate the best configuration on a held-out test set.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv(\"../../data/processed_data/case_1.csv\")\n",
    "X = df.drop(columns=\"class\")\n",
    "y_raw = df[\"class\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical columns for preprocessing\n",
    "num_cols = X.select_dtypes(include=[\"int\", \"float\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Build preprocessing pipelines\n",
    "numeric_pipeline = SkPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = SkPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, num_cols),\n",
    "    (\"cat\", categorical_pipeline, cat_cols),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing and encode labels\n",
    "X_proc = preprocessor.fit_transform(X)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "# Compute balanced class weights to handle imbalanced data\n",
    "class_weight = dict(enumerate(\n",
    "    compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "))\n",
    "\n",
    "# Split into train/val/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architectures to try (simple manual grid search)\n",
    "param_grid = [\n",
    "    {\"layers\": [64, 32], \"dropout\": 0.3, \"lr\": 1e-3},\n",
    "    {\"layers\": [128, 64, 32], \"dropout\": 0.5, \"lr\": 5e-4},\n",
    "    {\"layers\": [128, 64], \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train each configuration and track validation accuracy\n",
    "for params in param_grid:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(input_dim,)))\n",
    "\n",
    "    for units in params[\"layers\"]:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "        if params[\"dropout\"] > 0:\n",
    "            model.add(tf.keras.layers.Dropout(params[\"dropout\"]))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=params[\"lr\"]),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_acc = history.history[\"val_accuracy\"][-1]\n",
    "    results.append((params, val_acc))\n",
    "    print(f\"Params: {params} -> Val acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best based on validation accuracy\n",
    "best_params = max(results, key=lambda x: x[1])[0]\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Rebuild & retrain on full training set\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(input_dim,)))\n",
    "\n",
    "for units in best_params[\"layers\"]:\n",
    "    model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "    if best_params[\"dropout\"] > 0:\n",
    "        model.add(tf.keras.layers.Dropout(best_params[\"dropout\"]))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params[\"lr\"]),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=le.classes_)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dexire-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
